# cs231n Computer Vision Machine Learning
 
My solutions to the assignments of Stanford's computer vision course, cs231n. Going through the lectures and assignments was very rewarding and a great learning experience.

For assignments 2 and 3 you get the choice to use Tensorflow or Pytorch, I used Pytorch in all cases. The assignments are those from 2017 rather than the most recent ones, because this is the year the lectures were recorded.

By the end of the assignments, I had learnt enough to make past assignments a lot more elegant. If I was to do them again the code would probably be a lot cleaner.

Some of the highlights are presented below.

<p align="center">
  <img width="460" src="https://user-images.githubusercontent.com/63521540/90349584-b2295680-e03a-11ea-9b1e-a8ab0a892b76.PNG">
</p>

<p align="center">
  Experimenting with speedup when taking advantage of numpy's array broadcasting.
</p>

<p align="center">
  <img width="460" src="https://user-images.githubusercontent.com/63521540/90349743-2e239e80-e03b-11ea-89a7-24d85a5f71ab.PNG">
</p>

<p align="center">
  Visualizing the filters on the first layer of a convolutional neural network trained on the CIFAR10 dataset. Later, a convolutional neural network was trained to over 70% accuracy on the validation dataset, and 69% on the test set.
</p>

<p align="center">
  <img width="460" src="https://user-images.githubusercontent.com/63521540/90349939-d6d1fe00-e03b-11ea-8dd4-b691d18cced6.png">
</p>

<p align="center">
  Performing style transfer using the features extracted by SqueezeNet.
</p>

<p align="center">
  <img width="460" src="https://user-images.githubusercontent.com/63521540/90350088-3af4c200-e03c-11ea-92e0-3914eefe94bc.png">
</p>

<p align="center">
  Overfitting an LSTM for image captioning. At the top the caption outputted by the model makes sense. At the bottom, the caption of an image from the validation set is nonsense.
</p>
